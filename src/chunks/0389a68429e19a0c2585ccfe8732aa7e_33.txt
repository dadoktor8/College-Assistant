ut wouldn’t be sufficient to solve the problem well. Remember thatthe universal tension in machine learning is between optimization and generalization.The ideal model is one that stands right at the border between underfitting and over-fitting, between undercapacity and overcapacity. To figure out where this border lies,first you must cross it. To figure out how big a model you’ll need, you must develop a model that overfits.This is fairly easy, as you learned in chapter 5:1Add layers.2Make the layers bigger.3Train for more epochs.Always monitor the training loss and validation loss, as well as the training and valida-tion values for any metrics you care about. When you see that the model’s perfor-mance on the validation data begins to degrade, you’ve achieved overfitting. (continued)Choosing the right last-layer activation and loss function for your modelProblem typeLast-layer activationLoss functionBinary classificationsigmoid binary_crossentropyMulticlass, single-label classificationsoftmax categorical_crossentropyMulticlass, multilabel classificationsigmoid binary_crossentropy
165Deploy the model6.2.5 Regularize and tune your modelOnce you’ve achieved statistical power and you’re able to overfit, you know you’re on theright path. At this point, your goal becomes to maximize generalization performance