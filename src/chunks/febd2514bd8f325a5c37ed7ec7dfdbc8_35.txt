rfit to the validation process (eventhough no model is directly trained on any of the validation data). This makes theevaluation process less reliable. Once you’ve developed a satisfactory model configuration, you can train yourfinal production model on all the available data (training and validation) and evalu-ate it one last time on the test set. If it turns out that performance on the test set issignificantly worse than the performance measured on the validation data, this maymean either that your validation procedure wasn’t reliable after all, or that youbegan overfitting to the validation data while tuning the parameters of the model.In this case, you may want to switch to a more reliable evaluation protocol (such asiterated K-fold validation). 6.3 Deploy the modelYour model has successfully cleared its final evaluation on the test set—it’s ready to bedeployed and to begin its productive life.6.3.1 Explain your work to stakeholders and set expectationsSuccess and customer trust are about consistently meeting or exceeding people’sexpectations. The actual system you deliver is only half of that picture; the other halfis setting appropriate expectations before launch. The expectations of non-specialists towards AI systems are often unrealistic. Forexample, they might expect that the system “understands” its task and is capable of
166CHAPTER 6The universal workflow of machine learningexercising human-like common sense in the context of the task