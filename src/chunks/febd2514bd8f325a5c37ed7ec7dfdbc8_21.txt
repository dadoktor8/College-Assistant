etwork that’s popular with foodies. Come deployment time, feedback fromangry users starts rolling in: your app gets the answer wrong 8 times out of 10. What’sgoing on? Your accuracy on the test set was well over 90%! A quick look at user-uploadeddata reveals that mobile picture uploads of random dishes from random restaurantstaken with random smartphones look nothing like the professional-quality, well-lit,appetizing pictures you trained the model on: your training data wasn’t representative of theproduction data. That’s a cardinal sin—welcome to machine learning hell. If possible, collect data directly from the environment where your model will beused. A movie review sentiment classification model should be used on new IMDBreviews, not on Yelp restaurant reviews, nor on Twitter status updates. If you want torate the sentiment of a tweet, start by collecting and annotating actual tweets from asimilar set of users as those you’re expecting in production. If it’s not possible to trainon production data, then make sure you fully understand how your training and pro-duction data differ, and that you are actively correcting for these differences. A related phenomenon you should be aware of is concept drift. You’ll encounterconcept drift in almost all real-world problems, especially those that deal with user-generated data. Concept drift occurs when the properties of the production datachange over time, causing model accuracy to gradually decay