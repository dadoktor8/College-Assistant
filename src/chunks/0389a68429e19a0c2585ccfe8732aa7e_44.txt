a dry run on production data—the model behaved exactly as you expected.You’ve written unit tests as well as logging and status-monitoring code—perfect. Now it’stime to press the big red button and deploy to production. Even this is not the end. Once you’ve deployed a model, you need to keep moni-toring its behavior, its performance on new data, its interaction with the rest of theapplication, and its eventual impact on business metrics.Is user engagement in your online radio up or down after deploying the newmusic recommender system? Has the average ad click-through rate increasedafter switching to the new click-through-rate prediction model? Consider usingrandomized A/B testing to isolate the impact of the model itself from otherchanges: a subset of cases should go through the new model, while anothercontrol subset should stick to the old process. Once sufficiently many cases havebeen processed, the difference in outcomes between the two is likely attribut-able to the model.If possible, do a regular manual audit of the model’s predictions on productiondata. It’s generally possible to reuse the same infrastructure as for data annotation:send some fraction of the production data to be manually annotated, and com-pare the model’s predictions to the new annotations. For instance, you shoulddefinitely do this for the image search engine and the bad-cookie flagging system.