ly run the model on a GPU. To deploy a Keras model on a smartphone or embedded device, your go-to solutionis TensorFlow Lite (www.tensorflow.org/lite). It’s a framework for efficient on-devicedeep learning inference that runs on Android and iOS smartphones, as well as ARM64-based computers, Raspberry Pi, or certain microcontrollers. It includes a converter thatcan straightforwardly turn your Keras model into the TensorFlow Lite format. DEPLOYING A MODEL IN THE BROWSERDeep learning is often used in browser-based or desktop-based JavaScript applications.While it is usually possible to have the application query a remote model via a RESTAPI, there can be key advantages in having the model run directly in the browser, onthe user’s computer (utilizing GPU resources if they’re available). Use this setup whenYou want to offload compute to the end user, which can dramatically reduceserver costs.The input data needs to stay on the end user’s computer or phone. Forinstance, in our spam detection project, the web version and the desktop ver-sion of the chat app (implemented as a cross-platform app written in Java-Script) should use a locally run model.Your application has strict latency constraints. While a model running on the enduser’s laptop or smartphone is likely to be slower than one running on a largeGPU on your own server, you don’t have the extra 100 ms of network round trip