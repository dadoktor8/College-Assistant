32 d a t a .  I n  t h eexamples of classifying digits and predicting house prices, the data came in vectorizedform, so we were able to skip this step. VALUE NORMALIZATIONIn the MNIST digit-classification example from chapter 2, we started with image dataencoded as integers in the 0–255 range, encoding grayscale values. Before we fed thisdata into our network, we had to cast it to float32 and divide by 255 so we’d end upwith floating-point values in the 0–1 range. Similarly, when predicting house prices, westarted with features that took a variety of ranges—some features had small floating-point values, and others had fairly large integer values. Before we fed this data intoour network, we had to normalize each feature independently so that it had a stan-dard deviation of 1 and a mean of 0. In general, it isn’t safe to feed into a neural network data that takes relativelylarge values (for example, multi-digit integers, which are much larger than the ini-tial values taken by the weights of a network) or data that is heterogeneous (forexample, data where one feature is in the range 0–1 and another is in the range100–200). Doing so can trigger large gradient updates that will prevent the network
162CHAPTER 6The universal workflow of machine learningfrom converging. To make learning easier for your network, your data should havethe following characteristics:Take small values—Typically, most values should be in the 0–1 range