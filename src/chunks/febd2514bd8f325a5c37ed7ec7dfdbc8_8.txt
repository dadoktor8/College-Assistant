 manifold” of the training data. This is why deeplearning models can only make sense of inputs that are very close to whatthey’ve seen during training.The fundamental problem in machine learning is the tension between optimizationand generalization: to attain generalization, you must first achieve a good fit tothe training data, but improving your model’s fit to the training data will inevi-tably start hurting generalization after a while. Every single deep learning bestpractice deals with managing this tension.The ability of deep learning models to generalize comes from the fact that theymanage to learn to approximate the latent manifold of their data, and can thusmake sense of new inputs via interpolation.It’s essential to be able to accurately evaluate the generalization power of yourmodel while you’re developing it. You have at your disposal an array of evalua-tion methods, from simple holdout validation to K-fold cross-validation anditerated K-fold cross-validation with shuffling. Remember to always keep a com-pletely separate test set for final model evaluation, since information leaks fromyour validation data to your model may have occurred.When you start working on a model, your goal is first to achieve a model thathas some generalization power and that can overfit. Best practices for doingthis include tuning your learning rate and batch size, leveraging better architec-ture priors, increasing model capacity, or simply training longer