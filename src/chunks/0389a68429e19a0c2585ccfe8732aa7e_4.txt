 is preferred: dropout. Listing 5.14 Different weight regularizers available in KerasFigure 5.19 Effect of L2 weight regularization on validation loss
L1 regularizationSimultaneous L1 and L2 regularization
150CHAPTER 5Fundamentals of machine learningADDING DROPOUTDropout i s  o n e  o f  t h e  m o s t  e f f e c t i v e  a n d  m o s t  c o m m o n l y  u s e d  r e g u l a r i z a t i o n  t e c h -niques for neural networks; it was developed by Geoff Hinton and his students at theUniversity of Toronto. Dropout, applied to a layer, consists of randomly dropping out(setting to zero) a number of output features of the layer during training. Let’s say agiven layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a giveninput sample during training. After applying dropout, this vector will have a few zeroentries distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rateis the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5.At test time, no units are dropped out; instead, the layer’s output values are scaleddown by a factor equal to the dropout rate, to balance for the fact that more units areactive than at training time. C o n s i d e r  a  N u m P y  m a t r i x  c o n t a i n i n g  t h e  o u t p u t  o f  a  l a y e r ,  layer_output, ofshape (batch_size, features). At training time, we zero out at random a fraction ofthe values in the matrix:layer_output *= np.random