 constraints on the com-plexity of a model by forcing its weights to take only small values, which makes thedistribution of weight values more regular. This is called weight regularization, and it’sdone by adding to the loss function of the model a cost associated with having largeweights. This cost comes in two flavors:L1 regularization—The cost added is proportional to the absolute value of theweight coefficients (the L1 norm of the weights).L2 regularization—The cost added is proportional to the square of the value of theweight coefficients (the L2 norm of the weights). L2 regularization is also calledweight decay in the context of neural networks. Don’t let the different name con-fuse you: weight decay is mathematically the same as L2 regularization.In Keras, weight regularization is added by passing weight regularizer instances to layersas keyword arguments. Let’s add L2 weight regularization to our initial movie-reviewclassification model.from tensorflow.keras import regularizersmodel = keras.Sequential([    layers.Dense(16,                 kernel_regularizer=regularizers.l2(0.002),                 activation="relu"),    layers.Dense(16,                 kernel_regularizer=regularizers.l2(0.002),                 activation="relu"),    layers.Dense(1, activation="sigmoid")])model.compile(optimizer="rmsprop",              loss="binary_crossentropy",              metrics=["accuracy"])Listing 5.13 Adding L2 weight regularization to the model