f you ask me, it’s not the most difficult one. The hardest things inmachine learning are framing problems and collecting, annotating, and cleaningdata. So cheer up—what comes next will be easy in comparison!6.2.1 Prepare the dataAs you’ve learned before, deep learning models typically don’t ingest raw data. Datapreprocessing aims at making the raw data at hand more amenable to neural net-works. This includes vectorization, normalization, or handling missing values. Manypreprocessing techniques are domain-specific (for example, specific to text data orimage data); we’ll cover those in the following chapters as we encounter them inpractical examples. For now, we’ll review the basics that are common to all datadomains.VECTORIZATIONAll inputs and targets in a neural network must typically be tensors of floating-pointdata (or, in specific cases, tensors of integers or strings). Whatever data you need toprocess—sound, images, text—you must first turn into tensors, a step called data vec-torization. For instance, in the two previous text-classification examples in chapter 4,we started with text represented as lists of integers (standing for sequences of words),and we used one-hot encoding to turn them into a tensor of float32 d a t a .  I n  t h eexamples of classifying digits and predicting house prices, the data came in vectorizedform, so we were able to skip this step