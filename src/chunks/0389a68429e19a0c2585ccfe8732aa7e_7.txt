])history_dropout = model.fit(    train_data, train_labels,    epochs=20, batch_size=512, validation_split=0.4)Figure 5.21 shows a plot of the results. This is a clear improvement over the referencemodel—it also seems to be working much better than L2 regularization, since the low-est validation loss reached has improved.Listing 5.15 Adding dropout to the IMDB model
Figure 5.21 Effect of dropout on validation loss

152CHAPTER 5Fundamentals of machine learningTo recap, these are the most common ways to maximize generalization and preventoverfitting in neural networks:Get more training data, or better training data.Develop better features.Reduce the capacity of the model.Add weight regularization (for smaller models).Add dropout. SummaryThe purpose of a machine learning model is to generalize: to perform accuratelyon never-before-seen inputs. It’s harder than it seems.A deep neural network achieves generalization by learning a parametric modelthat can successfully interpolate between training samples—such a model can besaid to have learned the “latent manifold” of the training data. This is why deeplearning models can only make sense of inputs that are very close to whatthey’ve seen during training