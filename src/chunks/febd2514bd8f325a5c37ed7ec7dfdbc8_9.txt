ingthis include tuning your learning rate and batch size, leveraging better architec-ture priors, increasing model capacity, or simply training longer.As your model starts overfitting, your goal switches to improving generalizationthrough model regularization. You can reduce your model’s capacity, add dropoutor weight regularization, and use early stopping. And naturally, a larger or bet-ter dataset is always the number one way to help a model generalize.
153The universal workflowof machine learning
Our previous examples have assumed that we already had a labeled dataset to startfrom, and that we could immediately start training a model. In the real world, thisis often not the case. You don’t start from a dataset, you start from a problem. I m a g i n e  t h a t  y o u ’ r e  s t a r t i n g  y o u r  o w n  m a c h i n e  l e a r n i n g  c o n s u l t i n g  s h o p .  Y o uincorporate, you put up a fancy website, you notify your network. The projects startrolling in:A personalized photo search engine for a picture-sharing social network—type in “wedding” and retrieve all the pictures you took at weddings, withoutany manual tagging needed.Flagging spam and offensive text content among the posts of a buddingchat app.Building a music recommendation system for users of an online radio.Detecting credit card fraud for an e-commerce website