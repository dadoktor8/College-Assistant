eering—Filter out uninformative features (feature selection) and useyour knowledge of the problem to develop new features that are likely to be useful.Selecting the correct architecture priors—What type of model architecture will youuse? A densely connected network, a convnet, a recurrent neural network, aTransformer? Is deep learning even a good approach for the task, or should youuse something else?Selecting a good-enough training configuration—What loss function should you use?What batch size and learning rate?Picking the right loss functionIt’s often not possible to directly optimize for the metric that measures success ona problem. Sometimes there is no easy way to turn a metric into a loss function; lossfunctions, after all, need to be computable given only a mini-batch of data (ideally, aloss function should be computable for as little as a single data point) and must bedifferentiable (otherwise, you can’t use backpropagation to train your network). Forinstance, the widely used classification metric ROC AUC can’t be directly optimized.Hence, in classification tasks, it’s common to optimize for a proxy metric of ROC AUC,such as crossentropy. In general, you can hope that the lower the crossentropy gets,the higher the ROC AUC will be.The following table can help you choose a last-layer activation and a loss function fora few common problem types.
164CHAPTER 6The universal workflow of machine learning
For most problems, there are existing templates you can start from