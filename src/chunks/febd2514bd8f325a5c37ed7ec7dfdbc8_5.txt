 layer_output, ofshape (batch_size, features). At training time, we zero out at random a fraction ofthe values in the matrix:layer_output *= np.random.randint(0,h i g h =2,s i z e = l a y e r _ o u t p u t . s h a p e )   At test time, we scale down the output by the dropout rate. Here, we scale by 0.5(because we previously dropped half the units):layer_output *=0.5    Note that this process can be implemented by doing both operations at training timeand leaving the output unchanged at test time, which is often the way it’s imple-mented in practice (see figure 5.20):layer_output *= np.random.randint(0,h i g h =2,s i z e = l a y e r _ o u t p u t . s h a p e )    layer_output /=0.5    
This technique may seem strange and arbitrary. Why would this help reduce overfit-ting? Hinton says he was inspired by, among other things, a fraud-prevention mecha-nism used by banks. In his own words, “I went to my bank. The tellers kept changingand I asked one of them why. He said he didn’t know but they got moved around a lot.At training time, drops out 50% of the units in the outputAt test timeAt training timeNote that we’re scaling up rather than scaling down in this case.0.3* 20.60.20.70.20.11.90.51.50.00.31.00.00.31.20.00.050%dropout0.60.00.70.20.11.90.01.50.00.30.00.00.30.00.0Figure 5.20 Dropout applied to an activation matrix at training time, with rescaling happening during training. At test time the activation matrix is unchanged.