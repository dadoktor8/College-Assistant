nenvironment with strict constraints on available power and memory (smartphonesand embedded devices) or for applications with low latency requirements. You shouldalways seek to optimize your model before importing into TensorFlow.js or exportingit to TensorFlow Lite. There are two popular optimization techniques you can apply:Weight pruning—Not every coefficient in a weight tensor contributes equally tothe predictions. It’s possible to considerably lower the number of parametersin the layers of your model by only keeping the most significant ones. Thisreduces the memory and compute footprint of your model, at a small cost inperformance metrics. By deciding how much pruning you want to apply, youare in control of the trade-off between size and accuracy.Weight quantization—Deep learning models are trained with single-precisionfloating-point (float32) weights. However, it’s possible to quantize weights to8-bit signed integers (int8) to get an inference-only model that’s a quarter thesize but remains near the accuracy of the original model.The TensorFlow ecosystem includes a weight pruning and quantization toolkit (www.tensorflow.org/model_optimization) that is deeply integrated with the Keras API. 6.3.3 Monitor your model in the wildYou’ve exported an inference model, you’ve integrated it into your application, andyou’ve done a dry run on production data—the model behaved exactly as you expected.You’ve written unit tests as well as logging and status-monitoring code—perfect