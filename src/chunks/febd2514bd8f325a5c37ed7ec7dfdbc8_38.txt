s that can make the model faster and reduce its memory footprint. Let’s take a quick look at the different model deployment options you have available.DEPLOYING A MODEL AS A REST APIThis is perhaps the common way to turn a model into a product: install TensorFlow ona server or cloud instance, and query the model’s predictions via a REST API. Youcould build your own serving app using something like Flask (or any other Pythonweb development library), or use TensorFlow’s own library for shipping models asAPIs, called TensorFlow Serving (www.tensorflow.org/tfx/guide/serving). With Tensor-Flow Serving, you can deploy a Keras model in minutes.
167Deploy the model You should use this deployment setup whenThe application that will consume the model’s prediction will have reliableaccess to the internet (obviously). For instance, if your application is a mobileapp, serving predictions from a remote API means that the application won’t beusable in airplane mode or in a low-connectivity environment.The application does not have strict latency requirements: the request, infer-ence, and answer round trip will typically take around 500 ms.The input data sent for inference is not highly sensitive: the data will need tobe available on the server in a decrypted form, since it will need to be seen bythe model (but note that you should use SSL encryption for the HTTP requestand answer)