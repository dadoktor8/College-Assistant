 a dog breed classification task require spe-cialized knowledge. Meanwhile, annotating CT scans of bone fractures prettymuch requires a medical degree.If annotating the data requires specialized knowledge, can you train people todo it? If not, how can you get access to relevant experts?Do you, yourself, understand the way experts come up with the annotations? Ifyou don’t, you will have to treat your dataset as a black box, and you won’t be ableto perform manual feature engineering—this isn’t critical, but it can be limiting.If you decide to label your data in-house, ask yourself what software you will use to recordannotations. You may well need to develop that software yourself. Productive data anno-tation software will save you a lot of time, so it’s worth investing in it early in a project. BEWARE OF NON-REPRESENTATIVE DATAMachine learning models can only make sense of inputs that are similar to whatthey’ve seen before. As such, it’s critical that the data used for training should be repre-sentative of the production data. This concern should be the foundation of all yourdata collection work. Suppose you’re developing an app where users can take pictures of a plate of food tofind out the name of the dish. You train a model using pictures from an image-sharingsocial network that’s popular with foodies. Come deployment time, feedback fromangry users starts rolling in: your app gets the answer wrong 8 times out of 10