imizer="rmsprop",              loss="binary_crossentropy",              metrics=["accuracy"])Listing 5.13 Adding L2 weight regularization to the model
149Improving generalizationhistory_l2_reg = model.fit(    train_data, train_labels,    epochs=20, batch_size=512, validation_split=0.4)In the preceding listing, l2(0.002) means every coefficient in the weight matrix ofthe layer will add 0.002 * weight_coefficient_value ** 2 to the total loss of themodel. Note that because this penalty is only added at training time, the loss for thismodel will be much higher at training than at test time. Figure 5.19 shows the impact of the L2 regularization penalty. As you can see, themodel with L2 regularization has become much more resistant to overfitting than thereference model, even though both models have the same number of parameters.
As an alternative to L2 regularization, you can use one of the following Keras weightregularizers.fromtensorflow.kerasimportregularizersregularizers.l1(0.001)              regularizers.l1_l2(l1=0.001,l 2 =0.001)  Note that weight regularization is more typically used for smaller deep learning mod-els. Large deep learning models tend to be so overparameterized that imposing con-straints on weight values hasnâ€™t much impact on model capacity and generalization. Inthese cases, a different regularization technique is preferred: dropout. Listing 5.14 Different weight regularizers available in KerasFigure 5.19 Effect of L2 weight regularization on validation loss