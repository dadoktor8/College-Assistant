reference model.Listing 5.12 Version of the model with higher capacity
Figure 5.18 Original model vs. much larger model on IMDB review classification

148CHAPTER 5Fundamentals of machine learningThe bigger model starts overfitting almost immediately, after just one epoch, and itoverfits much more severely. Its validation loss is also noisier. It gets training loss nearzero very quickly. The more capacity the model has, the more quickly it can model thetraining data (resulting in a low training loss), but the more susceptible it is to overfit-ting (resulting in a large difference between the training and validation loss). ADDING WEIGHT REGULARIZATIONYou may be familiar with the principle of Occam’s razor: given two explanations forsomething, the explanation most likely to be correct is the simplest one—the one thatmakes fewer assumptions. This idea also applies to the models learned by neural net-works: given some training data and a network architecture, multiple sets of weightvalues (multiple models) could explain the data. Simpler models are less likely to over-fit than complex ones. A simple model in this context is a model where the distribution of parameter valueshas less entropy (or a model with fewer parameters, as you saw in the previous sec-tion). Thus, a common way to mitigate overfitting is to put constraints on the com-plexity of a model by forcing its weights to take only small values, which makes thedistribution of weight values more regular