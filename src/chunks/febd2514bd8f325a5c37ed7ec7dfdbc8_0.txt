147Improving generalizationAs you can see, the smaller model starts overfitting later than the reference model(after six epochs rather than four), and its performance degrades more slowly once itstarts overfitting. Now, let’s add to our benchmark a model that has much more capacity—far morethan the problem warrants. While it is standard to work with models that are signifi-cantly overparameterized for what they’re trying to learn, there can definitely be sucha thing as too much memorization capacity. You’ll know your model is too large if itstarts overfitting right away and if its validation loss curve looks choppy with high-variance (although choppy validation metrics could also be a symptom of using anunreliable validation process, such as a validation split that’s too small).model = keras.Sequential([    layers.Dense(512, activation="relu"),    layers.Dense(512, activation="relu"),    layers.Dense(1, activation="sigmoid")])model.compile(optimizer="rmsprop",              loss="binary_crossentropy",              metrics=["accuracy"])history_larger_model = model.fit(    train_data, train_labels,    epochs=20, batch_size=512, validation_split=0.4)Figure 5.18 shows how the bigger model fares compared with the reference model.Listing 5.12 Version of the model with higher capacity
Figure 5.18 Original model vs. much larger model on IMDB review classification
